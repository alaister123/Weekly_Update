{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caf24af",
   "metadata": {},
   "source": [
    "# Weekly Report Mar 30th 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0644a",
   "metadata": {},
   "source": [
    "## Completed\n",
    "\n",
    "### Papers Read\n",
    "\n",
    "Ge Luo, Hebi Li, Youbiao He and Forrest Sheng Bao, PrefScore: Pairwise Preference Learning for Reference-free Summarization Quality Assessment, COLING 2022, See you in Korea!\n",
    "\n",
    "Forrest Sheng Bao, Ge Luo, Hebi Li, Cen Chen, Yinfei Yang, Youbiao He, Minghui Qiu, A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling, NAACL 2022 (Trailer Video)\n",
    "\n",
    "Spurious Correlations in Reference-Free Evaluation of Text Generation\n",
    "\n",
    "Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation\n",
    "\n",
    "FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation\n",
    "\n",
    "RoMe: A Robust Metric for Evaluating Natural Language Generation\n",
    "\n",
    "Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons\n",
    "\n",
    "UniTE: Unified Translation Evaluation\n",
    "\n",
    "Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics, Findings\n",
    "\n",
    "Just Rank: Rethinking Evaluation with Word and Sentence Similarities, weakly related\n",
    "\n",
    "\n",
    "### I understood maybe like 25% of what is happening. I think I understood the basic idea of research in summarization evaluation metrics but I have read more papers. In addition I plan to do some actual coding and experimenting myself to get a better understanding\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "\n",
    "## Todos Next Week\n",
    "\n",
    "### Read\n",
    "\n",
    "A Survey of Evaluation Metrics Used for NLG Systems\n",
    "\n",
    "A Survey of Natural Language Generation\n",
    "\n",
    "A Tutorial on Evaluation Metrics used in Natural Language Generation\n",
    "\n",
    "Reread previous papers, see if can understand better\n",
    "\n",
    "Any other paper that I think may be useful for a better understanding for this area of research\n",
    "\n",
    "\n",
    "### Code\n",
    "\n",
    "The words ROUGE, METEOR and BLEU pops up a lot in the papers I have read. I'm going to check them out by coding some examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bf1d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
