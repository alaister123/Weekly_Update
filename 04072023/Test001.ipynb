{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(scores):\n",
    "    for i in scores[0]:\n",
    "        print(f'{i}:')\n",
    "        print('f1       ' + str(scores[0][i]['f']))\n",
    "        print('recall   ' + str(scores[0][i]['r']))\n",
    "        print('precision' + str(scores[0][i]['p']))\n",
    "        print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "f1       0.999999995\n",
      "recall   1.0\n",
      "precision1.0\n",
      "-------------------------------------\n",
      "rouge-2:\n",
      "f1       0.999999995\n",
      "recall   1.0\n",
      "precision1.0\n",
      "-------------------------------------\n",
      "rouge-l:\n",
      "f1       0.999999995\n",
      "recall   1.0\n",
      "precision1.0\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"I am a cat\"\n",
    "\n",
    "reference = \"I am a cat\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "f1       0.39999999680000003\n",
      "recall   0.25\n",
      "precision1.0\n",
      "-------------------------------------\n",
      "rouge-2:\n",
      "f1       0.0\n",
      "recall   0.0\n",
      "precision0.0\n",
      "-------------------------------------\n",
      "rouge-l:\n",
      "f1       0.39999999680000003\n",
      "recall   0.25\n",
      "precision1.0\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"cat cat cat cat\"\n",
    "reference = \"I am a cat\"\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "f1       0.6666666617283951\n",
      "recall   0.6\n",
      "precision0.75\n",
      "-------------------------------------\n",
      "rouge-2:\n",
      "f1       0.5714285665306124\n",
      "recall   0.5\n",
      "precision0.6666666666666666\n",
      "-------------------------------------\n",
      "rouge-l:\n",
      "f1       0.6666666617283951\n",
      "recall   0.6\n",
      "precision0.75\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"I am a kitten\"\n",
    "reference = \"I am a small cat\"\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "f1       0.4999999950000001\n",
      "recall   0.5\n",
      "precision0.5\n",
      "-------------------------------------\n",
      "rouge-2:\n",
      "f1       0.0\n",
      "recall   0.0\n",
      "precision0.0\n",
      "-------------------------------------\n",
      "rouge-l:\n",
      "f1       0.24999999500000009\n",
      "recall   0.25\n",
      "precision0.25\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"Gone are the days\"\n",
    "reference = \"The days are gone\"\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "f1       0.7999999950000002\n",
      "recall   0.8\n",
      "precision0.8\n",
      "-------------------------------------\n",
      "rouge-2:\n",
      "f1       0.749999995\n",
      "recall   0.75\n",
      "precision0.75\n",
      "-------------------------------------\n",
      "rouge-l:\n",
      "f1       0.7999999950000002\n",
      "recall   0.8\n",
      "precision0.8\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"I am a small mice\"\n",
    "reference = \"I am a small cat\"\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print_score(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3850322886878713"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"I am a kitten\"\n",
    "reference = \"I am a small cat\"\n",
    "sentence_bleu([hypothesis], reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668740304976422"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"I am a small mice\"\n",
    "reference = \"I am a small cat\"\n",
    "sentence_bleu([hypothesis.split(' ')], reference.split(' '), smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668740304976422"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"you am a small cat\"\n",
    "reference = \"I am a small cat\"\n",
    "sentence_bleu([hypothesis.split(' ')], reference.split(' '), smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21506254256566312"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"A small cat which I am not\"\n",
    "reference = \"I am not a small cat\"\n",
    "sentence_bleu([hypothesis.split(' ')], reference.split(' '), smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09554427922043669"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"Gone are the days\"\n",
    "reference = \"The days are gone\"\n",
    "sentence_bleu([hypothesis.split(' ')], reference.split(' '), smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09554427922043669"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"Elon Musk owns Tesla\"\n",
    "reference = \"Tesla is owned by Elon Musk\"\n",
    "sentence_bleu([hypothesis.split(' ')], reference.split(' '), smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4138243206810592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"It is a guide to action which ensures that the military always obeys the commands of the party\"\n",
    "reference = \"It is a guide to action that ensures that the military will forever heed Party commands\"\n",
    "sentence_bleu([hypothesis.split(' ')], reference.split(' '), smoothing_function=SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor([\"The cat sat on the mat\"],'on the mat sat the cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7398275988019579"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_meteor_score(reference1, hypothesis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1:\n",
      "f1       0.9090909041322315\n",
      "recall   1.0\n",
      "precision0.8333333333333334\n",
      "-------------------------------------\n",
      "rouge-2:\n",
      "f1       0.7272727223140496\n",
      "recall   0.8\n",
      "precision0.6666666666666666\n",
      "-------------------------------------\n",
      "rouge-l:\n",
      "f1       0.9090909041322315\n",
      "recall   1.0\n",
      "precision0.8333333333333334\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"the cat was found under the bed\"\n",
    "reference = \"the cat was under the bed\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "recall = 1.0\n",
    "precision = 0.8333333333333334\n",
    "print(2 * ((precision * recall)/(precision + recall)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implementation_rouge_1(ref, hyp):\n",
    "    ref_len = len(ref.split(' '))                              \n",
    "    \n",
    "    # number of words model predicts correctly in ground truth\n",
    "    true_positive = len(set(ref.split(' ')).intersection(set(hyp.split(' ')))) \n",
    "    \n",
    "    # number of words model predicts incorrectly in ground truth\n",
    "    false_positive = len(set(hyp.split(' '))) - true_positive                  \n",
    "    \n",
    "    \n",
    "    # number of words in ground truth model left out by model\n",
    "    false_negative = len(set(ref.split(' '))) - true_positive                  \n",
    "    \n",
    "    \n",
    "    \n",
    "    precision = true_positive / (false_positive + true_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    f1 = 2 * ((precision * recall)/(precision + recall))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f1)\n",
    "    print(recall)\n",
    "    print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "1.0\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "implementation_rouge_1(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 1.0, 'p': 0.8333333333333334, 'f': 0.9090909041322315}\n",
      "0.9090909090909091\n",
      "1.0\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"the cat was found under the bed\"\n",
    "reference = \"the cat was under the bed\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores[0]['rouge-1'])\n",
    "implementation_rouge_1(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 1.0, 'p': 0.5, 'f': 0.6666666622222223}\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"the tiny little cat was found under the big funny bed\"\n",
    "reference = \"the cat was under the bed\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores[0]['rouge-1'])\n",
    "implementation_rouge_1(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 0.7333333333333333, 'p': 0.6875, 'f': 0.7096774143600416}\n",
      "0.7096774193548386\n",
      "0.7333333333333333\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"It is a guide to action which ensures that the military always obeys the commands of the party\"\n",
    "reference = \"It is a guide to action that ensures that the military will forever heed Party commands\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores[0]['rouge-1'])\n",
    "implementation_rouge_1(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = \"to make people trustworthy you need to trust them\"\n",
    "reference = \"the way to make people trustworthy is to trust them\"\n",
    "\n",
    "reference = reference.split(' ')\n",
    "hypothesis = [hypothesis.split(' ')]\n",
    "\n",
    "sentence_bleu(hypothesis, reference, weights=(1, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def bleu_1_gram(hypo, refe):\n",
    "    \n",
    "    \n",
    "    intersec = [i for i in refe if i in hypo[0]]\n",
    "    res = len(intersec)/len(refe)\n",
    "    \n",
    "    # Brevity Penalty\n",
    "    BP = 1\n",
    "    if len(hypo[0]) <= len(refe):\n",
    "        BP = math.e ** (1 - (len(refe))/len(hypo[0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(intersec)    \n",
    "    print(res)\n",
    "    \n",
    "    \n",
    "    # Geometric Avg Percision but its just unigram so it the same\n",
    "    print(BP * math.exp(math.log(res)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'make', 'people', 'trustworthy', 'to', 'trust', 'them']\n",
      "0.7\n",
      "0.6263875217700589\n"
     ]
    }
   ],
   "source": [
    "bleu_1_gram(hypothesis, reference) # idk why result like this maybe BP penality is implemented incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
